import os
import re
import time
import traceback
from io import BytesIO
from urllib.parse import urljoin, urlparse
from datetime import datetime

import requests
from bs4 import BeautifulSoup
from docx import Document
from docx.shared import Inches
from docx.image.exceptions import UnrecognizedImageError

# å¯é¸ï¼šwebp -> pngï¼ˆæ²’è£ PIL ä¹Ÿèƒ½è·‘ï¼Œåªæ˜¯ webp å¯èƒ½è·³éï¼‰
try:
    from PIL import Image
    PIL_OK = True
except Exception:
    PIL_OK = False

OUT_DIR = r"F:\F\AI"
SLEEP_SEC = 0.35
MAX_MEDIA = 40


def safe_filename(name: str, max_len: int = 120) -> str:
    name = re.sub(r'[<>:"/\\|?*]', "_", str(name)).strip()
    name = re.sub(r"\s+", "_", name)          # âœ… ç©ºç™½ -> _
    name = re.sub(r"_+", "_", name)           # åˆä½µå¤šå€‹ _
    if len(name) > max_len:
        name = name[:max_len].rstrip("_")
    return name or "output"


def _browser_headers(referer: str = "https://www.threads.com/") -> dict:
    return {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/122.0.0.0 Safari/537.36"
        ),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
        "Accept-Language": "zh-TW,zh;q=0.9,en;q=0.8",
        "Referer": referer,
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
    }


def parse_threads_url_info(url: str) -> str:
    u = urlparse(url)
    parts = [p for p in u.path.split("/") if p]
    handle = ""
    post_id = ""
    # å¸¸è¦‹ï¼š/@handle/post/POSTID
    for i, p in enumerate(parts):
        if p.startswith("@"):
            handle = p.lstrip("@")
        if p == "post" and i + 1 < len(parts):
            post_id = parts[i + 1]
    if handle and post_id:
        return f"threads_{handle}_{post_id}"
    if post_id:
        return f"threads_{post_id}"
    return "threads_post"


def fetch_html_requests(url: str, timeout: int = 30) -> str:
    url = url.split("#", 1)[0]
    with requests.Session() as s:
        s.headers.update(_browser_headers())
        r = s.get(url, timeout=timeout, allow_redirects=True)
        if not r.encoding or r.encoding.lower() == "iso-8859-1":
            r.encoding = r.apparent_encoding or "utf-8"
        return r.text


def fetch_playwright_bundle(url: str):
    """
    âœ… ç”¨ Playwright æŠ“ï¼š
    - å¯è¦– DOM æ­£æ–‡ï¼ˆç”¨ã€Œæœ€é•·å¯è¦–æ–‡å­—å€å¡Šã€ç­–ç•¥ï¼‰
    - è²¼æ–‡å€å¡Šå…§åœ–ç‰‡ï¼ˆcurrentSrc/src/srcsetï¼‰
    - æ•´é æˆªåœ–ï¼ˆä¿åº•ï¼‰
    """
    from playwright.sync_api import sync_playwright

    headless = os.environ.get("HEADLESS", "1") != "0"

    with sync_playwright() as p:
        browser = p.chromium.launch(
            headless=headless,
            args=[
                "--no-sandbox",
                "--disable-setuid-sandbox",
                "--disable-dev-shm-usage",
                "--disable-blink-features=AutomationControlled",
            ],
        )
        context = browser.new_context(
            user_agent=_browser_headers()["User-Agent"],
            locale="zh-TW",
            viewport={"width": 1366, "height": 900},
        )
        page = context.new_page()

        page.goto(url, wait_until="domcontentloaded", timeout=60000)
        page.wait_for_timeout(2200)

        # è§¸ç™¼ lazy-load
        try:
            page.mouse.wheel(0, 1800)
            page.wait_for_timeout(1200)
        except Exception:
            pass

        # âœ… 1) å…ˆæŠ“ã€Œå¯è¦–æ­£æ–‡ã€ï¼šæ‰¾ main å…§æœ€é•·çš„å¯è¦–æ–‡å­—å€å¡Š
        post_text = ""
        try:
            post_text = page.evaluate(
                """() => {
                    const norm = (s) => (s || '')
                        .replace(/\\r/g, '')
                        .replace(/\\n{3,}/g, '\\n\\n')
                        .trim();

                    const isBad = (el) => {
                        if (!el) return true;
                        const tag = (el.tagName || '').toLowerCase();
                        if (['script','style','noscript','svg'].includes(tag)) return true;
                        const role = (el.getAttribute && el.getAttribute('role')) || '';
                        if (role && ['navigation','banner','dialog'].includes(role)) return true;
                        return false;
                    };

                    const main = document.querySelector('main') || document.body;
                    const cand = [];
                    const walker = document.createTreeWalker(main, NodeFilter.SHOW_ELEMENT, null);
                    while (walker.nextNode()) {
                        const el = walker.currentNode;
                        if (isBad(el)) continue;

                        // æ’é™¤æ˜é¡¯ UIï¼šæŒ‰éˆ•ã€è¼¸å…¥ç­‰
                        const tag = (el.tagName || '').toLowerCase();
                        if (['button','input','textarea','select'].includes(tag)) continue;

                        // åªå–å¯è¦‹å…ƒç´ 
                        const st = window.getComputedStyle(el);
                        if (!st || st.display === 'none' || st.visibility === 'hidden') continue;

                        const txt = norm(el.innerText);
                        if (!txt) continue;

                        // æ–‡å­—å¤ªçŸ­ä¸è¦
                        if (txt.length < 80) continue;

                        // é¿å…æŠ“åˆ°æ•´é ï¼šåªä¿ç•™åŒ…å«æ›è¡Œ/æ®µè½çš„
                        const score = txt.length + (txt.includes('\\n') ? 120 : 0);

                        cand.push({score, txt});
                    }

                    cand.sort((a,b) => b.score - a.score);

                    // å–ç¬¬ä¸€å€‹æœ€åƒæ­£æ–‡çš„ï¼ˆé€šå¸¸å°±æ˜¯è²¼æ–‡ï¼‰
                    if (cand.length) {
                        // å†åšä¸€é»éæ¿¾ï¼šä¸è¦åŒ…å«å¤ªå¤šç¶²ç«™å›ºå®šå­—
                        const blacklist = ['Meta.ai', 'Cookie', 'ç™»å…¥', 'Log in', 'Sign up'];
                        for (const c of cand) {
                            const bad = blacklist.some(k => c.txt.includes(k));
                            if (!bad) return c.txt;
                        }
                        return cand[0].txt;
                    }

                    return '';
                }"""
            )
        except Exception:
            post_text = ""

        # âœ… 2) å†æŠ“ã€Œè²¼æ–‡å€å¡Šå…§åœ–ç‰‡ã€ï¼šå…ˆé–å®š mainï¼Œå†æŠ“æ‰€æœ‰ images çš„ currentSrc/srcset
        img_urls = []
        try:
            img_urls = page.evaluate(
                """() => {
                    const out = new Set();
                    const main = document.querySelector('main') || document.body;

                    const imgs = Array.from(main.querySelectorAll('img'));
                    for (const img of imgs) {
                        const w = img.naturalWidth || 0;
                        const h = img.naturalHeight || 0;

                        // éæ¿¾ï¼šå¤ªå°çš„å¤šåŠæ˜¯é ­åƒ/åœ–ç¤º
                        if (w && h && (w < 120 || h < 120)) continue;

                        if (img.currentSrc) out.add(img.currentSrc);
                        if (img.src) out.add(img.src);

                        const ss = img.getAttribute('srcset');
                        if (ss) {
                            ss.split(',')
                              .map(s => s.trim().split(' ')[0])
                              .forEach(u => { if (u) out.add(u); });
                        }
                    }

                    // meta åœ–
                    const og = document.querySelector('meta[property="og:image"]');
                    if (og && og.content) out.add(og.content);

                    return Array.from(out);
                }"""
            )
        except Exception:
            img_urls = []

        # âœ… 3) æ•´é æˆªåœ–ä¿åº•
        screenshot_bytes = None
        try:
            screenshot_bytes = page.screenshot(full_page=True)
        except Exception:
            screenshot_bytes = None

        html = page.content()
        browser.close()

    # éæ¿¾ï¼šdata: / svg / ico
    cleaned = []
    seen = set()
    for u in (img_urls or []):
        u = (u or "").strip()
        if not u or u.startswith("data:"):
            continue
        pth = urlparse(u).path.lower()
        if pth.endswith(".svg") or pth.endswith(".ico"):
            continue
        if u in seen:
            continue
        seen.add(u)
        cleaned.append(u)

    return html, cleaned, screenshot_bytes, (post_text or "").strip()


def extract_meta_title_and_date(html: str):
    soup = BeautifulSoup(html, "lxml")

    def meta(prop=None, name=None):
        if prop:
            t = soup.find("meta", attrs={"property": prop})
            if t and t.get("content"):
                return t["content"].strip()
        if name:
            t = soup.find("meta", attrs={"name": name})
            if t and t.get("content"):
                return t["content"].strip()
        return ""

    title = meta(prop="og:title") or meta(name="twitter:title") or ""

    # æ—¥æœŸï¼šæ‹¿ä¸åˆ°å°±ä»Šå¤©
    date8 = ""
    for prop in ["article:published_time", "og:published_time", "og:updated_time"]:
        t = meta(prop=prop)
        if t:
            m = re.search(r"(20\d{2})-(\d{1,2})-(\d{1,2})", t)
            if m:
                y, mo, d = m.group(1), int(m.group(2)), int(m.group(3))
                date8 = f"{y}{mo:02d}{d:02d}"
                break
    if not date8:
        date8 = datetime.now().strftime("%Y%m%d")

    return title.strip(), date8


def download_image_bytes(session: requests.Session, img_url: str):
    try:
        r = session.get(img_url, timeout=30)
        r.raise_for_status()
        ctype = (r.headers.get("Content-Type") or "").lower()
        if "image" not in ctype:
            return None, ctype
        return r.content, ctype
    except Exception:
        return None, ""


def maybe_convert_webp_to_png_bytes(img_bytes: bytes, ctype: str, img_url: str):
    low_ct = (ctype or "").lower()
    ext = os.path.splitext(urlparse(img_url).path.lower())[1]
    is_webp = ("image/webp" in low_ct) or (ext == ".webp")
    if not is_webp or not PIL_OK:
        return None
    try:
        im = Image.open(BytesIO(img_bytes))
        out = BytesIO()
        im.convert("RGB").save(out, format="PNG")
        return out.getvalue()
    except Exception:
        return None


def add_picture_to_doc(doc: Document, img_bytes: bytes, width_inches: float = 6.3) -> bool:
    try:
        doc.add_picture(BytesIO(img_bytes), width=Inches(width_inches))
        return True
    except UnrecognizedImageError:
        return False
    except Exception:
        return False


def main():
    url = input("è«‹è¼¸å…¥ Threads è²¼æ–‡ç¶²å€ï¼š\n").strip()
    if not url:
        print("âŒ æœªè¼¸å…¥ç¶²å€ï¼ŒçµæŸ")
        return

    os.makedirs(OUT_DIR, exist_ok=True)

    # âœ… Threadsï¼šç›´æ¥ç”¨ Playwright bundleï¼ˆæœ€ç©©ï¼‰
    html, img_urls, screenshot_bytes, dom_text = fetch_playwright_bundle(url)

    meta_title, date8 = extract_meta_title_and_date(html)
    fallback_title = parse_threads_url_info(url)

    # âœ… æ¨™é¡Œï¼šmeta_title æœ‰ç”¨å°±ç”¨ï¼Œå¦å‰‡ç”¨ç¶²å€æ¨å°
    title = meta_title if meta_title and meta_title.lower() != "threads" else fallback_title

    # âœ… æ­£æ–‡ï¼šå„ªå…ˆç”¨ DOM æŠ½åˆ°çš„å¯è¦–æ–‡å­—ï¼ˆä½ ç¾åœ¨ç¼ºçš„å°±æ˜¯é€™æ®µï¼‰
    text = dom_text.strip()

    out_path = os.path.join(OUT_DIR, f"{date8}_{safe_filename(title)}.docx")

    doc = Document()
    doc.add_heading(title, level=0)
    doc.add_paragraph(f"ä¾†æºç¶²å€ï¼š{url}")
    doc.add_paragraph(f"å»ºæª”æ—¥æœŸï¼š{date8}")
    doc.add_paragraph("")

    if text:
        doc.add_paragraph(text)
    else:
        doc.add_paragraph("ï¼ˆæœªæˆåŠŸæŠ½å–åˆ°æ­£æ–‡ï¼Œå¯èƒ½è²¼æ–‡æ¬Šé™å—é™æˆ–éœ€ç™»å…¥ï¼‰")

    # âœ… æ’å…¥åœ–ç‰‡ï¼ˆå¾ DOM æŠ“åˆ°çš„ imgï¼‰
    img_urls = (img_urls or [])[:MAX_MEDIA]
    img_count = 0

    with requests.Session() as s:
        s.headers.update(_browser_headers(referer="https://www.threads.com/"))

        for img_url in img_urls:
            img, ctype = download_image_bytes(s, img_url)
            if not img:
                continue

            converted = maybe_convert_webp_to_png_bytes(img, ctype, img_url)
            ok = False
            if converted:
                ok = add_picture_to_doc(doc, converted)
            if not ok:
                ok = add_picture_to_doc(doc, img)

            if ok:
                img_count += 1
                time.sleep(SLEEP_SEC)

    # âœ… å¦‚æœä»ç„¶æ²’æŠ“åˆ°ä»»ä½•åœ–ç‰‡ï¼šæ’å…¥æ•´é æˆªåœ–ï¼ˆä¿åº•ä¸€å®šæœ‰ï¼‰
    if img_count == 0 and screenshot_bytes:
        doc.add_page_break()
        doc.add_heading("è²¼æ–‡æˆªåœ–", level=1)
        if add_picture_to_doc(doc, screenshot_bytes, width_inches=6.8):
            img_count = 1

    doc.save(out_path)
    print(f"âœ… å®Œæˆï¼š{out_path}")
    print(f"ğŸ“Œ åœ–ç‰‡ï¼š{img_count} å¼µ")


if __name__ == "__main__":
    try:
        main()
    except Exception:
        print("âŒ ç¨‹å¼ç™¼ç”Ÿæœªè™•ç†ä¾‹å¤–ï¼š")
        traceback.print_exc()
        # âœ… ä¸æš«åœï¼Œç›´æ¥å› CMD
